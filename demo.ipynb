{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T08:05:00.182066Z",
     "iopub.status.busy": "2026-01-18T08:05:00.181639Z",
     "iopub.status.idle": "2026-01-18T08:05:00.186354Z",
     "shell.execute_reply": "2026-01-18T08:05:00.185484Z",
     "shell.execute_reply.started": "2026-01-18T08:05:00.182033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/kaggle/working/MADM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T08:05:00.187718Z",
     "iopub.status.busy": "2026-01-18T08:05:00.187312Z",
     "iopub.status.idle": "2026-01-18T08:05:00.424688Z",
     "shell.execute_reply": "2026-01-18T08:05:00.423946Z",
     "shell.execute_reply.started": "2026-01-18T08:05:00.187696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done: BraTS pretrain dataset ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# ====== PATH ======\n",
    "src_root = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
    "dst_root = \"/kaggle/working/brats_pretrain\"\n",
    "\n",
    "os.makedirs(dst_root, exist_ok=True)\n",
    "\n",
    "# ====== SPLIT ======\n",
    "cases = sorted([\n",
    "    d for d in os.listdir(src_root)\n",
    "    if os.path.isdir(os.path.join(src_root, d))\n",
    "])\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(cases)\n",
    "\n",
    "train_ratio = 0.8\n",
    "split_idx = int(len(cases) * train_ratio)\n",
    "\n",
    "splits = {\n",
    "    \"train\": cases[:split_idx],\n",
    "    \"val\": cases[split_idx:]\n",
    "}\n",
    "\n",
    "modalities = [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n",
    "\n",
    "# ====== REFORMAT ======\n",
    "for split, case_list in splits.items():\n",
    "    for case in case_list:\n",
    "        src_case = os.path.join(src_root, case)\n",
    "        dst_case = os.path.join(dst_root, split, case)\n",
    "\n",
    "        os.makedirs(dst_case, exist_ok=True)\n",
    "\n",
    "        for f in os.listdir(src_case):\n",
    "            for m in modalities:\n",
    "                if f.endswith(f\"{m}.nii.gz\"):\n",
    "                    shutil.copy(\n",
    "                        os.path.join(src_case, f),\n",
    "                        os.path.join(dst_case, f\"{m}.nii.gz\")\n",
    "                    )\n",
    "\n",
    "print(\"✅ Done: BraTS pretrain dataset ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/input\n",
    "!ls /kaggle/input/cmlaimadm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/cmlaimadm /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/working/cmlaimadm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/cmlaimadm/dataloader_scripts/load_pet_2_5D.py\") as f:\n",
    "    print(f.read()[:1500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/cmlaimadm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_ac(data, mask):\n",
    "    non_zero_data = data[mask == True]\n",
    "    non_zero_mean = np.mean(non_zero_data)\n",
    "    data = data / non_zero_mean\n",
    "    data = np.tanh(data / 5)\n",
    "    return data\n",
    "\n",
    "def get_mask(nac, num_blurred=5, threshold=0.05):\n",
    "    blurred_nac = np.copy(nac)\n",
    "    for _ in range(num_blurred):\n",
    "        blurred_nac = ndimage.gaussian_filter(blurred_nac, sigma=1)\n",
    "    mask = nac > threshold\n",
    "    return mask\n",
    "\n",
    "\n",
    "class LoadPetSlices(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        axis=\"z\",\n",
    "        load_adj=8,\n",
    "        out_size=192,\n",
    "        use_every_n_slice=2,\n",
    "        seed=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert axis in [\"x\", \"y\", \"z\"]\n",
    "        random.seed(seed)\n",
    "\n",
    "        self.axis = axis\n",
    "        self.load_adj = load_adj\n",
    "        self.out_size = out_size\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.ids = [f[:-9] for f in os.listdir(os.path.join(root_dir, \"5NAC\"))]\n",
    "        self.slice_map = []\n",
    "\n",
    "        for pid in self.ids:\n",
    "            nac_path = os.path.join(root_dir, \"5NAC\", pid + \"5_NAC.nii\")\n",
    "            shape = nib.load(nac_path).shape\n",
    "\n",
    "            depth = shape[2] if axis == \"z\" else shape[1] if axis == \"y\" else shape[0]\n",
    "\n",
    "            for s in range(load_adj, depth - load_adj, use_every_n_slice):\n",
    "                self.slice_map.append((pid, s))\n",
    "\n",
    "        print(f\"[Dataset] total slices = {len(self.slice_map)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_map)\n",
    "\n",
    "    def extract_25d(self, vol, center_idx):\n",
    "        slices = []\n",
    "        for i in range(center_idx - self.load_adj, center_idx + self.load_adj + 1):\n",
    "            if self.axis == \"z\":\n",
    "                slices.append(vol[:, :, i])\n",
    "            elif self.axis == \"y\":\n",
    "                slices.append(vol[:, i, :])\n",
    "            else:\n",
    "                slices.append(vol[i, :, :])\n",
    "        return torch.from_numpy(np.stack(slices, axis=0))  # (C,H,W)\n",
    "    def __getitem__(self, idx):\n",
    "        pid, slice_idx = self.slice_map[idx]\n",
    "\n",
    "    # -------------------------\n",
    "    # Load volumes\n",
    "    # -------------------------\n",
    "        nac = nib.load(\n",
    "            os.path.join(self.root_dir, \"5NAC\", pid + \"5_NAC.nii\")\n",
    "        ).get_fdata().astype(np.float32)\n",
    "\n",
    "        ac = nib.load(\n",
    "            os.path.join(self.root_dir, \"100AC\", pid + \"100_AC.nii\")\n",
    "        ).get_fdata().astype(np.float32)\n",
    "\n",
    "    # -------------------------\n",
    "    # Normalize with mask\n",
    "    # -------------------------\n",
    "        mask = get_mask(nac)\n",
    "        nac = normalize_ac(nac, mask)\n",
    "        ac  = normalize_ac(ac, mask)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2.5D NAC \n",
    "    # -------------------------\n",
    "        nac_25d = self.extract_25d(nac, slice_idx)   \n",
    "        nac_25d = torch.from_numpy(nac_25d).float()\n",
    "\n",
    "        center = self.load_adj\n",
    "\n",
    "    \n",
    "        cond = torch.cat(\n",
    "            [\n",
    "                nac_25d[:center],        \n",
    "                nac_25d[center + 1:],   \n",
    "            ],\n",
    "            dim=0\n",
    "        )                               \n",
    "\n",
    "    # -------------------------\n",
    "    # AC target (2D)\n",
    "    # -------------------------\n",
    "        if self.axis == \"z\":\n",
    "            ac_2d = ac[:, :, slice_idx]\n",
    "        elif self.axis == \"y\":\n",
    "            ac_2d = ac[:, slice_idx, :]\n",
    "        else:\n",
    "            ac_2d = ac[slice_idx, :, :]\n",
    "\n",
    "        ac_2d = torch.from_numpy(ac_2d).unsqueeze(0).float()\n",
    "\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            cond  = torch.flip(cond, dims=[1])\n",
    "            ac_2d = torch.flip(ac_2d, dims=[1])\n",
    "        if random.random() < 0.5:\n",
    "            cond  = torch.flip(cond, dims=[2])\n",
    "            ac_2d = torch.flip(ac_2d, dims=[2])\n",
    "\n",
    "\n",
    "        if idx == 0:\n",
    "            print(\"AC   :\", ac_2d.shape)\n",
    "            print(\"COND :\", cond.shape)\n",
    "\n",
    "        assert cond.shape[0] == self.load_adj * 2\n",
    "        assert ac_2d.shape[0] == 1\n",
    "        return ac_2d, {\"cond\": cond}\n",
    "\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    batch_size,\n",
    "    root_dir,\n",
    "    axis=\"z\",\n",
    "    load_adj=8,\n",
    "    out_size=192,\n",
    "):\n",
    "    dataset = LoadPetSlices25D(\n",
    "        root_dir=root_dir,\n",
    "        axis=axis,\n",
    "        load_adj=load_adj,\n",
    "        out_size=out_size,\n",
    "        use_every_n_slice=2,   \n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,       \n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        yield from loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install mpi4py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "\n",
    "src_root = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
    "dst_root = \"/kaggle/working/NAC_data\"\n",
    "\n",
    "splits = {\n",
    "    \"train\": os.listdir(src_root)[:50],\n",
    "    \"val\": os.listdir(src_root)[50:70],\n",
    "}\n",
    "\n",
    "os.makedirs(dst_root, exist_ok=True)\n",
    "\n",
    "for split, cases in splits.items():\n",
    "    for sub in [\"5NAC\", \"100AC\"]:\n",
    "        os.makedirs(os.path.join(dst_root, split, sub), exist_ok=True)\n",
    "\n",
    "    for case in cases:\n",
    "        case_dir = os.path.join(src_root, case)\n",
    "        if not os.path.isdir(case_dir):\n",
    "            continue\n",
    "\n",
    "        # chọn modality\n",
    "        nac_path = [f for f in os.listdir(case_dir) if \"t1.nii\" in f][0]\n",
    "        ac_path  = [f for f in os.listdir(case_dir) if \"t1ce.nii\" in f][0]\n",
    "\n",
    "        shutil.copy(\n",
    "            os.path.join(case_dir, nac_path),\n",
    "            os.path.join(dst_root, split, \"5NAC\", f\"{case}_5_NAC.nii\")\n",
    "        )\n",
    "        shutil.copy(\n",
    "            os.path.join(case_dir, ac_path),\n",
    "            os.path.join(dst_root, split, \"100AC\", f\"{case}_100_AC.nii\")\n",
    "        )\n",
    "\n",
    "print(\"✅ Dataset mapped to NAC_data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# ===============================\n",
    "# Train diffusion model\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.resample import create_named_schedule_sampler\n",
    "from guided_diffusion.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    args_to_dict,\n",
    ")\n",
    "from guided_diffusion.train_util import TrainLoop\n",
    "from dataloader_scripts.load_pet_2_5D import LoadValData, load_data\n",
    "class Args:\n",
    "    schedule_sampler = \"uniform\"\n",
    "    resume_checkpoint = \"\"\n",
    "    use_fp16 = False\n",
    "    fp16_scale_growth = 1e-3\n",
    "    # 2.5D setting\n",
    "    train_axis = \"z\"# 3 lan cho x,y,z\n",
    "    load_adj = 2\n",
    "    out_channels = 1\n",
    "\n",
    "    # path\n",
    "    logdir = \"checkpoint_z\"\n",
    "    data_root = \"NAC_data\"\n",
    "\n",
    "args = Args()\n",
    "\n",
    "for k, v in model_and_diffusion_defaults().items():\n",
    "    setattr(args, k, v)\n",
    "args.batch_size = 1\n",
    "args.microbatch = -1\n",
    "args.lr = 1e-4\n",
    "args.weight_decay = 0.0\n",
    "args.lr_anneal_steps = 1000\n",
    "\n",
    "args.log_interval = 50\n",
    "args.save_interval = 500\n",
    "\n",
    "args.ema_rate = \"0.0\"\n",
    "          # tắt EMA\n",
    "args.use_fp16 = False\n",
    "\n",
    "# diffusion\n",
    "args.diffusion_steps = 25#50\n",
    "args.noise_schedule = \"linear\"\n",
    "\n",
    "# model I/O\n",
    "args.in_channels = 1 + (2 * args.load_adj + 1)\n",
    "args.out_channels = 1\n",
    "args.image_size = 128\n",
    "\n",
    "# safe UNet config\n",
    "args.model_channels = 32\n",
    "args.num_res_blocks = 1#2\n",
    "args.channel_mult = \"1,2,4\"  \n",
    "args.attention_resolutions = \"9999\"\n",
    "\n",
    "args.use_scale_shift_norm = False\n",
    "args.resblock_updown = False\n",
    "args.use_checkpoint = False\n",
    "\n",
    "\n",
    "dist_util.setup_dist()\n",
    "logger.configure(dir=args.logdir)\n",
    "\n",
    "logger.log(\"creating model and diffusion...\")\n",
    "args.channel_mult = \",\".join(map(str, args.channel_mult)) \\\n",
    "    if isinstance(args.channel_mult, (list, tuple)) else args.channel_mult\n",
    "\n",
    "args.attention_resolutions = \",\".join(map(str, args.attention_resolutions)) \\\n",
    "    if isinstance(args.attention_resolutions, (list, tuple)) else args.attention_resolutions\n",
    "model, diffusion = create_model_and_diffusion(\n",
    "    **args_to_dict(args, model_and_diffusion_defaults().keys())\n",
    ")\n",
    "model.to(dist_util.dev())\n",
    "\n",
    "schedule_sampler = create_named_schedule_sampler(\n",
    "    args.schedule_sampler, diffusion\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Data loader\n",
    "# -------------------------------\n",
    "logger.log(\"creating data loader...\")\n",
    "\n",
    "train_dir = os.path.join(args.data_root, \"train\")\n",
    "val_dir   = os.path.join(args.data_root, \"val\")\n",
    "\n",
    "data = load_data(\n",
    "    batch_size=args.batch_size,\n",
    "    root_dir=train_dir,\n",
    "    axis=args.train_axis,\n",
    "    load_adj=args.load_adj,\n",
    ")\n",
    "\n",
    "val_data = LoadValData(\n",
    "    root_dir=val_dir,\n",
    "    axis=args.train_axis,\n",
    "    load_adj=args.load_adj,\n",
    ")\n",
    "_orig_training_losses = diffusion.training_losses\n",
    "def safe_training_losses(model, x_start, cond, t, *args, **kwargs):\n",
    "    if not th.is_tensor(t):\n",
    "        t = th.tensor(t, device=x_start.device)\n",
    "    t = th.clamp(t, 0, diffusion.num_timesteps - 1)\n",
    "    return _orig_training_losses(model, x_start, cond, t, *args, **kwargs)\n",
    "\n",
    "diffusion.training_losses = safe_training_losses\n",
    "\n",
    "# -------------------------------\n",
    "# Train\n",
    "# -------------------------------\n",
    "logger.log(\"training...\")\n",
    "train_loop = TrainLoop(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    data=data,\n",
    "    val_data=val_data,\n",
    "    batch_size=args.batch_size,\n",
    "    microbatch=args.microbatch,\n",
    "    lr=args.lr,\n",
    "    ema_rate=args.ema_rate,\n",
    "    log_interval=args.log_interval,\n",
    "    save_interval=args.save_interval,\n",
    "    resume_checkpoint=args.resume_checkpoint,\n",
    "    use_fp16=args.use_fp16,\n",
    "    fp16_scale_growth=args.fp16_scale_growth,\n",
    "    schedule_sampler=schedule_sampler,\n",
    "    weight_decay=args.weight_decay,\n",
    "    lr_anneal_steps=args.lr_anneal_steps,\n",
    "    logdir=args.logdir,#save\n",
    ")\n",
    "# def save_model(model, logdir, axis, step):\n",
    "#     os.makedirs(logdir, exist_ok=True)\n",
    "#     save_path = os.path.join(logdir, f\"model_{axis}_step{step}.pt\")\n",
    "#     th.save(model.state_dict(), save_path)\n",
    "#     logger.log(f\"[SAVE] Model saved to {save_path}\")\n",
    "logger.log(\"training...\")\n",
    "train_loop.run_loop()\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def export_infer_ckpt(train_ckpt, save_path):\n",
    "    state = torch.load(train_ckpt, map_location=\"cpu\")\n",
    "\n",
    "    infer_ckpt = {\n",
    "        \"model\": state, \n",
    "    }\n",
    "\n",
    "    torch.save(infer_ckpt, save_path)\n",
    "    print(f\"[OK] Exported inference ckpt → {save_path}\")\n",
    "\n",
    "\n",
    "export_infer_ckpt(\n",
    "    \"checkpoint_y/best_model000500.pt\",\n",
    "    \"checkpoint_y/infer_model_y.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Validation & Sampling – 2.5D → 3D (x, y, z)\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import nibabel as nib\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    args_to_dict,\n",
    ")\n",
    "\n",
    "from dataloader_scripts.load_pet_2_5D import LoadTestData\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "\n",
    "    batch_size = 8\n",
    "    clip_denoised = True\n",
    "    use_ddim = False\n",
    "    prior_start_t = 25\n",
    "\n",
    "    data_root = \"NAC_data\"\n",
    "    save_root = \"MADM\"\n",
    "\n",
    "\n",
    "    load_adj = 2\n",
    "\n",
    "    in_channels = 1 + (2 * load_adj + 1)   # = 6 ✅\n",
    "    out_channels = 1\n",
    "    image_size = 128\n",
    "\n",
    "\n",
    "    model_channels = 136\n",
    "    channel_mult = (1,2,4,4)\n",
    "    num_res_blocks = 2\n",
    "    attention_resolutions = (16,)\n",
    "    \n",
    "    use_scale_shift_norm = False\n",
    "    resblock_updown = True\n",
    "    use_checkpoint = False\n",
    "    use_fp16 = False\n",
    "\n",
    "\n",
    "    diffusion_steps = 25\n",
    "    noise_schedule = \"linear\"\n",
    "\n",
    "\n",
    "    model_dirs = {\n",
    "        \"x\": \"checkpoint_x\",\n",
    "        \"y\": \"checkpoint_y\",\n",
    "        \"z\": \"checkpoint_z\",\n",
    "    }\n",
    "\n",
    "    axes = [\"x\", \"y\", \"z\"]\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "\n",
    "defaults = model_and_diffusion_defaults()\n",
    "for k, v in defaults.items():\n",
    "    if not hasattr(args, k):\n",
    "        setattr(args, k, v)\n",
    "\n",
    "args.in_channels = 1 + (2 * args.load_adj + 1)\n",
    "args.out_channels = 1\n",
    "args.image_size = 128\n",
    "\n",
    "\n",
    "args.diffusion_steps = 25\n",
    "args.noise_schedule = \"linear\"\n",
    "\n",
    "dist_util.setup_dist()\n",
    "logger.configure()\n",
    "\n",
    "device = dist_util.dev()\n",
    "logger.log(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "test_dir = os.path.join(args.data_root, \"val\")\n",
    "test_data = LoadTestData(\n",
    "    root_dir=test_dir,\n",
    "    load_adj=args.load_adj,\n",
    ")\n",
    "\n",
    "logger.log(f\"Number of test cases: {len(test_data)}\")\n",
    "\n",
    "for idx in range(len(test_data)):\n",
    "    logger.log(f\"\\n===== Sampling case {idx + 1}/{len(test_data)} =====\")\n",
    "    test_data.idx = idx\n",
    "\n",
    "    axis_predictions = {}\n",
    "\n",
    "\n",
    "\n",
    "    for axis in args.axes:\n",
    "        logger.log(f\"--- Axis: {axis}\")\n",
    "\n",
    "        test_data = LoadTestData(\n",
    "        root_dir=test_dir,\n",
    "        axis=axis,\n",
    "        load_adj=args.load_adj\n",
    "        )\n",
    "\n",
    "        args.channel_mult = \",\".join(map(str, args.channel_mult)) \\\n",
    "            if isinstance(args.channel_mult, (list, tuple)) else args.channel_mult\n",
    "\n",
    "        args.attention_resolutions = \",\".join(map(str, args.attention_resolutions)) \\\n",
    "            if isinstance(args.attention_resolutions, (list, tuple)) else args.attention_resolutions\n",
    "\n",
    "        model, diffusion = create_model_and_diffusion(\n",
    "            **args_to_dict(args, defaults.keys())\n",
    "        )\n",
    "\n",
    "        ckpt_path = os.path.join(\n",
    "            args.model_dirs[axis],\n",
    "            f\"infer_model_{axis}.pt\"\n",
    "        )\n",
    "        \n",
    "        logger.log(f\"Loading checkpoint: {ckpt_path}\")\n",
    "\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "        model.to(dist_util.dev())\n",
    "        model.eval()\n",
    "        model.requires_grad_(False)\n",
    "\n",
    "        H = args.image_size\n",
    "        W = args.image_size\n",
    "        D = test_data.get_zsize()\n",
    "\n",
    "        shape = (H, W, D)\n",
    "\n",
    "\n",
    "        with th.no_grad():\n",
    "            sample = diffusion.p_sample_loop(\n",
    "                model=model,\n",
    "                test_data=test_data,\n",
    "                shape=shape,\n",
    "                batch_size=args.batch_size,\n",
    "                start_t=args.prior_start_t,\n",
    "                clip_denoised=args.clip_denoised,\n",
    "                model_kwargs={},\n",
    "            )\n",
    "\n",
    "        axis_predictions[axis] = sample.cpu().numpy()\n",
    "\n",
    "        # free memory\n",
    "        del model\n",
    "        th.cuda.empty_cache()\n",
    "\n",
    "    # ======================================================\n",
    "    # Fuse 3 axis → final 3D volume\n",
    "    # ======================================================\n",
    "    logger.log(\"Fusing x, y, z predictions...\")\n",
    "\n",
    "    final_pred = (\n",
    "        axis_predictions[\"x\"] +\n",
    "        axis_predictions[\"y\"] +\n",
    "        axis_predictions[\"z\"]\n",
    "    ) / 3.0\n",
    "\n",
    "    final_pred[final_pred < 0] = 0\n",
    "\n",
    "\n",
    "    # ======================================================\n",
    "    # Save NIfTI\n",
    "    # ======================================================\n",
    "    out_dir = os.path.join(\n",
    "        args.save_root,\n",
    "        f\"adj{args.load_adj}_xyz_fusion\"\n",
    "    )\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"{test_data.get_name(idx)}_pred.nii.gz\"\n",
    "    )\n",
    "\n",
    "    nii = nib.Nifti1Image(final_pred.astype(np.float32), affine=np.eye(4))\n",
    "    nib.save(nii, out_path)\n",
    "\n",
    "    logger.log(f\"Saved: {out_path}\")\n",
    "\n",
    "logger.log(\"\\n✅ Sampling & validation finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "gt_path = test_data.get_gt_path(idx)  # hoặc tự ghép path\n",
    "gt_nii = nib.load(gt_path)\n",
    "gt = gt_nii.get_fdata()\n",
    "mae = np.mean(np.abs(final_pred - gt))\n",
    "psnr_val = psnr(gt, final_pred, data_range=gt.max() - gt.min())\n",
    "ssim_vals = []\n",
    "for i in range(gt.shape[2]):  # duyệt theo trục z\n",
    "    ssim_i = ssim(\n",
    "        gt[:, :, i],\n",
    "        final_pred[:, :, i],\n",
    "        data_range=gt.max() - gt.min()\n",
    "    )\n",
    "    ssim_vals.append(ssim_i)\n",
    "\n",
    "ssim_val = np.mean(ssim_vals)\n",
    "logger.log(\n",
    "    f\"[METRIC] MAE={mae:.4f} | PSNR={psnr_val:.2f} | SSIM={ssim_val:.4f}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 751906,
     "sourceId": 1299795,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9191725,
     "sourceId": 14431069,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
